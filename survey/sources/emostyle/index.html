<!DOCTYPE html>
<html lang="en">
<head>
  <title>EmoStyle</title>
  <meta name="description" content="EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters</h1>
    <!-- <h4>NeurIPS 2020 (oral presentation)</h4> -->
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a><nobr>Bita Azari*</nobr></a> &emsp;
        <a><nobr>Angelica Lim*</nobr></a> &emsp;
      </h4>
      *Simon Fraser University, <nobr>Burnaby</nobr>, <nobr>BC</nobr>, Canada
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/samples.png" alt="samples.png" class="text-center" style="width: 100%; max-width: 1100px">
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://openaccess.thecvf.com/content/WACV2024/papers/Azari_EmoStyle_One-Shot_Facial_Expression_Editing_Using_Continuous_Emotion_Parameters_WACV_2024_paper.pdf">Paper</a>
    <a class="label label-info" href="https://github.com/bihamta/emostyle">Code<span style="font-size: smaller; font-weight: normal;"> </span></a>
    <!-- <a class="label label-info" href="https://www.youtube.com/embed/j20MBc1hWGQ">Video</a>
    <a class="label label-info" href="resrc/dtic_long.pptx">Slides</a>
    <a class="label label-info" href="resrc/ref.bib">BibTeX</a> -->
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    Recent studies have achieved impressive results in face generation and editing of facial expressions.
    However, existing approaches either generate a discrete number of facial expressions or have limited control over the emotion of the output image.
    To overcome this limitation, we introduced EmoStyle, a method to edit facial expressions based on valence and arousal, two continuous emotional parameters that can specify a broad range of emotions.
    EmoStyle is designed to separate emotions from other facial characteristics and to edit the face to display a desired emotion.
    We employ the pre-trained generator from StyleGAN2, taking advantage of its rich latent space.
    We also proposed an adapted inversion method to be able to apply our system on out-of-StyleGAN2 domain (OOD) images in a one-shot manner.
    The qualitative and quantitative evaluations show that our approach has the capability to synthesize a wide range of expressions to output high-resolution images.
  </p>

  <h3>Approach</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <img src="resrc/workflow.png" alt="workflow.png" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <p><strong>Phase 1 - Training EmoExtract:</strong> We train the <strong>EmoExtract</strong> and up-sampling modules (green) by alternating Emotion Variation with random emotion parameters from the valence-arousal space (top),
        with Emotion Reconstruction of the input face (bottom). Five auxiliary losses are used for this purpose, as indicated by the dashed lines.
        The Inversion module is employed to extract the latent code <i>w</i> of the input image <i>I<sub>input</sub></i>.
        The <strong>EmoExtract</strong> module is trained to determine the necessary modifications <i>d</i> that should be applied to a latent code <i>w</i>.
        Note that <i>d</i> should result in 0 for the Emotion Reconstruction segment.
        The final latent code is generated by adding <i>d</i> to the original latent code <i>w</i>.
        Finally, the StyleGAN2 generator is used to create our desired image.</p>
      <p><strong>Phase 2 - Fine-tuning StyleGAN2:</strong> we freeze the <strong>EmoExtract</strong> module trained previously and fine-tune our StyleGAN2 component.
        Our inputs during this phase are emotion parameters and one out-of-domain face.
        First, we determine the face's latent code utilizing an inversion framework to extract the latent code in the StyleGAN2 <i>W</i> space,
        then perform a fine-tuning step.
        <!-- step inspired from <cite>roich2022pivotal</cite>. -->
      </p>
  </div>

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Results on StyleGAN images</u></h4>
      <img src="resrc/more.png" alt="results.ng" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div style="text-align: center; padding-left: 1rem; padding-right: 1rem; padding-bottom: 1rem;">
    <h4><u>Results on real images</u></h4>
    <div style="display: inline-block; width: calc(50% - 10px); box-sizing: border-box; padding-right: 10px;">
      <img src="resrc/barak_obama.gif" alt="barak_obama.gif" style="width: 100%;">
    </div>
    <div style="display: inline-block; width: calc(50% - 10px); box-sizing: border-box; padding-left: 10px;">
      <img src="resrc/taylor_swift.gif" alt="taylor_swift.gif" style="width: 100%;">
    </div>
  </div>
  

  <h3>Resources</h3>
  <hr/>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
          @inproceedings{azari2024emostyle,
            title={EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters},
            author={Azari, Bita and Lim, Angelica},
            booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
            pages={6385--6394},
            year={2024}
          }
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2104.14575">Monnier et al. - Unsupervised Layered Image Decomposition into Object
        Prototypes (arXiv 2021)</a>
    </li>
  </ul>

  <h4>Previous works on deep transformations</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1908.04725">Deprelle et al. - Learning elementary structures for 3D shape
        generation and matching (NeurIPS 2019)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1806.05228">Groueix et al. - 3D-CODED: 3D Correspondences by Deep Deformation (ECCV
        2018)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1802.05384">Groueix et al. - AtlasNet: A Papier-Mache Approach to Learning 3D
        Surface Generation (CVPR 2018)</a>
    </li>
  </ul> -->


  <!-- <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported in part by <a href="https://enherit.enpc.fr/">ANR project EnHerit</a> ANR-17-CE23-0008,
    project Rapid Tabasco, gifts  from  Adobe and HPC resources from GENCI-IDRIS (Grant 2020-AD011011697). We thank 
    Bryan Russell, Vladimir Kim, Matthew Fisher, Fran&#231;ois Darmon, Simon Roburin, David Picard, Michael 
    Ramamonjisoa, Vincent Lepetit, Elliot Vincent, Jean Ponce, William Peebles and Alexei Efros for inspiring 
    discussions and valuable feedback.
  </p>
</div> -->
<!-- 
<div class="container" id="license">
  <h3>MIT License</h3>
  <pre>
      MIT License

      Copyright (c) [Year] [Your Name]

      Permission is hereby granted, free of charge, to any person obtaining a copy
      of this software and associated documentation files (the "Software"), to deal
      in the Software without restriction, including without limitation the rights
      to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
      copies of the Software, and to permit persons to whom the Software is
      furnished to do so, subject to the following conditions:

      The above copyright notice and this permission notice shall be included in all
      copies or substantial portions of the Software.

      THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
      IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
      FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
      AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
      LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
      OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
      SOFTWARE.
  </pre>
</div> -->

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
